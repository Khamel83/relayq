# RelayQ Node-Local Environment Configuration Template
# Copy this file to ~/.config/relayq/env and customize for your runner

# =============================================================================
# ASR BACKEND SELECTION
# =============================================================================
# Choose the backend for audio transcription:
# - local: Use local Whisper/whisper.cpp (requires model files)
# - openai: Use OpenAI's Whisper API (requires API key, costs money)
# - router: Use router API like OpenRouter (requires API key, costs money)
ASR_BACKEND=local

# =============================================================================
# LOCAL WHISPER CONFIGURATION
# =============================================================================
# Whisper model to use for local transcription
# Available models: tiny, base, small, medium, large
# tiny: Fastest, least accurate (~32MB)
# base: Good balance (~142MB)
# small: Better accuracy (~466MB)
# medium: Even better accuracy (~1.5GB)
# large: Best accuracy (~2.9GB)
WHISPER_MODEL=base

# Path where Whisper models are stored
WHISPER_MODEL_PATH=/opt/models/whisper

# =============================================================================
# OPENAI API CONFIGURATION
# =============================================================================
# OpenAI API key for transcription services
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI model to use (currently only whisper-1 is available)
OPENAI_MODEL=whisper-1

# =============================================================================
# ROUTER API CONFIGURATION (OpenRouter, etc.)
# =============================================================================
# Router API key for accessing multiple model providers
# Get from: https://openrouter.ai/keys
# ROUTER_API_KEY=sk-or-your-router-api-key-here

# Router API base URL
ROUTER_BASE_URL=https://openrouter.ai/api/v1

# Router model to use for transcription
# Examples: openai/whisper-1, anthropic/claude-3-opus (if supported)
ROUTER_MODEL=openai/whisper-1

# =============================================================================
# FILE STORAGE CONFIGURATION
# =============================================================================
# Path for storing temporary files during processing
TEMP_DIR=/tmp

# Path for storing final outputs
OUTPUT_DIR=/tmp/relayq-outputs

# =============================================================================
# NAS CONFIGURATION (Optional via Tailscale)
# =============================================================================
# Mount point for Network Attached Storage via Tailscale
# This allows access to private data stores from runners
# NAS_MOUNT=/mnt/nas

# Path to media files on NAS
# NAS_MEDIA_PATH=/mnt/nas/media

# Path for storing processed files on NAS
# NAS_OUTPUT_PATH=/mnt/nas/processed

# =============================================================================
# RUNNER-SPECIFIC CONFIGURATION
# =============================================================================
# Mac mini specific settings
if [[ "$(hostname)" == *"mac"* ]] || [[ "$(uname)" == "Darwin" ]]; then
    # Mac mini has more resources, can handle larger models
    WHISPER_MODEL=small
    MAX_CONCURRENT_JOBS=2
fi

# Raspberry Pi 4 specific settings
if [[ "$(hostname)" == *"rpi4"* ]] || [[ "$(uname -m)" == "aarch64" ]]; then
    # RPi4 has limited resources, use smaller models
    WHISPER_MODEL=base
    MAX_CONCURRENT_JOBS=2

    # Consider using cloud API for better performance
    # ASR_BACKEND=openai
fi

# Raspberry Pi 3 specific settings
if [[ "$(hostname)" == *"rpi3"* ]]; then
    # RPi3 is very limited, use smallest model or cloud API
    WHISPER_MODEL=tiny
    MAX_CONCURRENT_JOBS=1

    # Recommended to use cloud API for better performance
    ASR_BACKEND=openai
fi

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Log level for job scripts
# Options: DEBUG, INFO, WARN, ERROR
LOG_LEVEL=INFO

# Log file location (optional)
# LOG_FILE=/var/log/relayq/jobs.log

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================
# File permissions for output files
OUTPUT_FILE_PERMISSIONS=644

# Directory permissions for temp directories
TEMP_DIR_PERMISSIONS=700

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================
# Maximum file size to process (in MB)
MAX_FILE_SIZE_MB=1000

# Maximum job duration (in seconds)
MAX_JOB_DURATION=14400  # 4 hours

# Number of CPU cores to use for processing (0 = use all available)
CPU_CORES=0

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example 1: Mac mini with local processing (most private)
# ASR_BACKEND=local
# WHISPER_MODEL=small
# WHISPER_MODEL_PATH=/opt/models/whisper

# Example 2: RPi4 with cloud processing (better performance)
# ASR_BACKEND=openai
# OPENAI_API_KEY=sk-your-key-here

# Example 3: Mixed approach with cost optimization
# ASR_BACKEND=local
# WHISPER_MODEL=base
# OPENAI_API_KEY=sk-your-key-here  # Fallback for large files

# =============================================================================
# NOTES
# =============================================================================
# - Never commit this file to version control
# - Keep API keys secure and rotate regularly
# - Adjust model sizes based on available RAM
# - Monitor API usage and costs
# - Test different backends to find optimal configuration